##Etat de l’art du référencement en 2016 
*Paul Sanches, [impactseo](http://impactseo.fr) et [seohackers](http://seohackers.fr/)*
####Google donne des réponses, il faut être la réponse
Avant l'hégémonie de Google Search, c'était assez facile de faire du SEO : il suffisait de faire du spam et peu importait la qualité du contenu du site. C'est que les autres moteurs de recherche fonctionnaient simplement sur les metadata : il suffisait de rentrer "mp3", "porn" pour faire remonter son site. Tout ça a changé avec le *PageRank* de google, fondé sur la popularité du site c'est-à-dire en partie du nombre de liens qui renvoient vers ce site.
Ces dernières années, les publicités sponsorisées de Google sont passées de 3 à 4 en première page, laissant moins de place pour le référencement naturel. Ainsi le premier résultat de l'algorithme du moteur de recherche arrive en cinquième ligne. Même chose pour les nouveaux services que Google développe (le cadre à droite dans les recherches sur un nom de personne par exemple) qui fait que __Google devient de plus en plus un moteur de réponse__ dans le but de garder l'internaute sur google seul, sans l'envoyer vers le site du producteur de contenu. Il faut donc __devenir la réponse que Google va aller chercher__ à droite et à gauche. Pour ce faire, il faut passer par la __microdata__.

Il y a toujours eu un triptyque de fonctionnement dans le SEO, quelles que soient ses évolutions : 
1. technical seo
2. onpage seo
3. offiste seo
Paul Sanches note que c'est le technical seo qui aujourd'hui est largement dominant. 

####Les recommandations (techniques) de Google
Les recommandations de Google pour être bien placé dans Google Search sont :
- la vitesse de chargement de la page 
- avoir une version mobile, Google AMP (*Accelerated Mobile Pages*) est l'outil maison (pages en cache chez Google bien évidemment, pas folle la guêpe)
- https à tel point qu'**au 1er janvier 2017, Chrome va rendre le passage en https obligatoire**.
 
Pour le onpage Google recommande de travailler avec Ajax, de favoriser le rich media, etc. Toutes ces recommandations sont d'ordre finalement techniques. 
Pour tester la qualité, on fait de plus en plus appel à des outils de crawl (BOTIFY par exemple) permettant en outre d'indiquer au crawler de Google qu'il est pas nécessaire de recrawler certaines pages. *Prerender.io*, *BromBone*, *seo.js* et *seo4.ajax* sont autant d'outils permettant de rendre son site crawlable par Google selon certaines opérations. Il y a également des pratiques moins nobles comme payer des gens pour qu'ils fasse tomber les sites concurrents en position derrière soi.
####Stratégies de contenu
Pour améliorer le référencement, on ne peut plus utiliser les vieilles ficelles des annuaires et communiqués de presse mais on peut axer son action sur les stratégies de contenu **// cocon sémantique** pour multiplier les portes d'entrée dans son site. Le *content marketing* à ce titre est une branche qui utilise des méthodes comme l'infographie, la petite application, les top, les vidéos *do it yourself* etc. (du *linkbaiting* en gros) pour multiplier les portes d'entrée et buzzer, pour attirer du monde sur son site (pour faire de la conversion, du lien, etc.). Ainsi des entreprises de bricolage qui font des vidéos en playlist sur YouTube pour savoir comment monter une étagère etc.

Quelle conclusion tirer de ces recommandations ? **En imposant ces conditions, Google force les producteurs de contenu à faire son travail en mettant en avant les résultats les plus intéressants** (aucun intérêt à faire remonter une page contact ni pour Google ni pour le site crawlé). 
####Stratégies de netlinking
La première chose à faire dans ce type de stratégie et qui mange pas de pain c'est de faire du partenariat avec des sites amis (aufeminin et marmiton appartiennent au même groupe par exemple). Ensuite, comme ailleurs, il suffit d'analyser ce que font les concurrents et pour ça il y a différents outils comme : 
- *ahrefs*
- *SEObserver* 

On peut également acheter des liens avec *Rocketslinks* bien que Google n'aime pas ça et pénalise ceux qui utilisent cette méthode. La raison est toute simple : Google préfère qu'on le paye lui et son *AdWords*.
Les **noms de domaines expirés** sont aussi à considérer puisque Google fait davantage "confiance" aux sites anciens. Il suffit alors de faire un renvoi vers son site une fois l'ancien site remonté. Est-ce que ça pollue le web comme on l'entend dire ? Non pas tout à fait, pas vraiment, car ces anciens sites sont pas destinés à remonter dans les résultats google mais à soutenir la remontée du site principal. 